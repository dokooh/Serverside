#!/usr/bin/env python3
"""
Model Preview Script
====================
Shows the list of models that will be tested before running the full benchmark suite.
This helps verify configuration and provides an overview of what will be downloaded/tested.

Usage:
    python preview_models.py
"""

import json
import sys
from pathlib import Path
from typing import Dict, List
import logging

# Add current directory to path for imports
sys.path.append(str(Path(__file__).parent))

from model_downloader import ModelDownloader
from model_tester import ModelTester

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

def format_size(size_gb: float) -> str:
    """Format size in a human-readable way"""
    if size_gb < 1:
        return f"{size_gb * 1024:.0f} MB"
    else:
        return f"{size_gb:.1f} GB"

def print_banner():
    """Print a nice banner"""
    print("=" * 80)
    print("ðŸ¤– HUGGING FACE MODEL BENCHMARK SUITE")
    print("ðŸ“‹ Model Preview & Configuration Summary")
    print("=" * 80)
    print()

def preview_models():
    """Preview all models that will be tested"""
    try:
        # Initialize components
        print("ðŸ”§ Initializing model downloader...")
        downloader = ModelDownloader()
        
        print("ðŸ”§ Initializing model tester...")
        tester = ModelTester()
        
        print()
        print("ðŸ“Š CONFIGURED MODELS:")
        print("-" * 50)
        
        total_estimated_size = 0
        model_count = 0
        
        # Display each model configuration
        for model_key, config in downloader.model_configs.items():
            model_count += 1
            print(f"\n{model_count}. ðŸ¤– {model_key.upper()}")
            print(f"   ðŸ“¦ Repository: {config['hf_repo']}")
            print(f"   ðŸ·ï¸  Type: {config.get('type', 'text-generation')}")
            print(f"   ðŸ“ Estimated Size: {format_size(config.get('estimated_size_gb', 1.0))}")
            print(f"   âš™ï¸  Quantization: {'âœ… Q2_K preferred' if config.get('prefer_q2k') else 'âœ… 4-bit preferred' if config.get('quantized_alternatives') else 'âŒ Full precision only'}")
            
            # Add alternatives if available
            quantized_alts = config.get('quantized_alternatives', [])
            if quantized_alts:
                print(f"   ðŸ”„ Quantized Alternatives:")
                for i, alt_repo in enumerate(quantized_alts, 1):
                    print(f"      {i}. {alt_repo}")
            
            total_estimated_size += config.get('estimated_size_gb', 1.0)
        
        print()
        print("=" * 50)
        print(f"ðŸ“ˆ SUMMARY:")
        print(f"   â€¢ Total Models: {model_count}")
        print(f"   â€¢ Estimated Total Size: {format_size(total_estimated_size)}")
        print(f"   â€¢ Cache Directory: {downloader.models_dir}")
        print("=" * 50)
        
        print()
        print("ðŸ§ª TEST PROMPTS OVERVIEW:")
        print("-" * 30)
        
        # Show prompt categories
        print(f"ðŸ“ General Text Prompts: {len(tester.test_prompts)} prompts")
        print(f"ðŸ‘ï¸  Vision Prompts: {len(tester.vision_prompts)} prompts")
        print(f"ðŸ“„ Document/OCR Prompts: {len(tester.document_prompts)} prompts")
        print(f"â“ Image QA Prompts: {len(tester.image_qa_prompts)} prompts")
        
        total_text_prompts = len(tester.test_prompts)
        total_vision_prompts = len(tester.vision_prompts) + len(tester.document_prompts) + len(tester.image_qa_prompts)
        
        print()
        print("ðŸŽ¯ TESTING STRATEGY:")
        print("-" * 20)
        print(f"   â€¢ Text-only models: {total_text_prompts} prompts each")
        print(f"   â€¢ Vision models: 9 mixed prompts each (vision + OCR + QA)")
        print(f"   â€¢ Max prompts per model: 5 (for efficiency)")
        
        print()
        print("ðŸ“Š BENCHMARK METRICS:")
        print("-" * 20)
        print("   â€¢ â±ï¸  Response Time (seconds)")
        print("   â€¢ ðŸ§  Memory Usage (MB)")
        print("   â€¢ ðŸ”¥ GPU Utilization (%)")
        print("   â€¢ ðŸ“ˆ Token Generation Rate")
        print("   â€¢ ðŸ’¾ Model Loading Time")
        
        print()
        print("ðŸ“ OUTPUT FILES:")
        print("-" * 15)
        print("   â€¢ logs/benchmark_YYYYMMDD_HHMMSS.log")
        print("   â€¢ cache/download_results.json")
        print("   â€¢ reports/model_performance_report.html")
        print("   â€¢ reports/benchmark_results.json")
        
        print()
        print("ðŸš€ READY TO START!")
        print("=" * 80)
        print("To run the full benchmark suite:")
        print("   python main_orchestrator.py")
        print()
        print("To run individual components:")
        print("   python model_downloader.py    # Download models only")
        print("   python model_tester.py        # Test existing models")
        print("   python report_generator.py    # Generate reports only")
        print("=" * 80)
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ Error during model preview: {e}")
        print(f"\nðŸ”§ TROUBLESHOOTING:")
        print(f"   â€¢ Make sure all dependencies are installed: pip install -r requirements.txt")
        print(f"   â€¢ Check if you have sufficient disk space ({format_size(total_estimated_size)} estimated)")
        print(f"   â€¢ Verify internet connection for model downloads")
        print(f"   â€¢ Error details: {str(e)}")
        return False

def show_detailed_model_info():
    """Show detailed information about each model"""
    print("\nðŸ” DETAILED MODEL INFORMATION:")
    print("=" * 60)
    
    downloader = ModelDownloader()
    
    for model_key, config in downloader.model_configs.items():
        print(f"\nðŸ“¦ {model_key.upper()}:")
        print(f"   Primary: {config['hf_repo']}")
        
        # Show quantized alternatives
        quantized_alts = config.get('quantized_alternatives', [])
        if quantized_alts:
            print(f"   Quantized options: {len(quantized_alts)} available")
            for alt in quantized_alts:
                print(f"     â€¢ {alt}")
        
        # Try to get more info about the selected model
        try:
            repo_id = config['hf_repo']
            print(f"   ðŸŽ¯ Selected: {repo_id}")
            
            # Show what type of model files we expect (all are text-generation models now)
            print(f"   ðŸ“ Expected files: model weights, tokenizer")
            print(f"   ðŸš€ Capabilities: Text generation, Question answering")
                
        except Exception as e:
            print(f"   âš ï¸  Could not resolve best variant: {e}")

def main():
    """Main preview function"""
    print_banner()
    
    # Show basic model preview
    if not preview_models():
        sys.exit(1)
    
    # Ask if user wants detailed info
    print("\n" + "="*50)
    response = input("ðŸ” Show detailed model information? [y/N]: ").strip().lower()
    if response in ['y', 'yes']:
        show_detailed_model_info()
    
    print("\nðŸŽ‰ Preview complete! Ready to start benchmarking.")

if __name__ == "__main__":
    main()