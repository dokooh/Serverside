{
  "timestamp": "2025-09-24 16:23:59",
  "system_info": {
    "cpu_percent": 5.7,
    "memory_used_gb": 25.276126861572266,
    "memory_total_gb": 31.751651763916016,
    "gpu_available": true,
    "gpu_count": 1,
    "gpu_memory_used_gb": 0.0,
    "gpu_memory_total_gb": 4.0,
    "python_version": "3.13.1 (tags/v3.13.1:0671451, Dec  3 2024, 19:06:28) [MSC v.1942 64 bit (AMD64)]",
    "pytorch_version": "2.7.1+cu118",
    "transformers_version": "4.57.0.dev0",
    "cuda_available": true,
    "recommended_device": "cuda"
  },
  "overall_stats": {
    "models_tested": 3,
    "successful_downloads": 3,
    "models_with_successful_benchmarks": 0,
    "total_benchmark_runs": 3,
    "successful_benchmark_runs": 0
  },
  "model_summaries": {
    "llama-3.2-1b": {
      "status": "all_benchmarks_failed",
      "model_info": {
        "repo_id": "meta-llama/Llama-3.2-1B",
        "size_gb": 1.0,
        "files": [],
        "path": "models\\llama-3.2-1b",
        "type": "standard",
        "status": "partial",
        "message": "Standard model download needs implementation"
      },
      "benchmark_count": 1,
      "errors": [
        "Converting from SentencePiece and Tiktoken failed, if a converter for SentencePiece is available, provide a model path with a SentencePiece tokenizer.model file.Currently available slow->fast converters: ['AlbertTokenizer', 'BartTokenizer', 'BarthezTokenizer', 'BertTokenizer', 'BigBirdTokenizer', 'BlenderbotTokenizer', 'CamembertTokenizer', 'CLIPTokenizer', 'CodeGenTokenizer', 'ConvBertTokenizer', 'DebertaTokenizer', 'DebertaV2Tokenizer', 'DistilBertTokenizer', 'DPRReaderTokenizer', 'DPRQuestionEncoderTokenizer', 'DPRContextEncoderTokenizer', 'ElectraTokenizer', 'FNetTokenizer', 'FunnelTokenizer', 'GPT2Tokenizer', 'HerbertTokenizer', 'LayoutLMTokenizer', 'LayoutLMv2Tokenizer', 'LayoutLMv3Tokenizer', 'LayoutXLMTokenizer', 'LongformerTokenizer', 'LEDTokenizer', 'LxmertTokenizer', 'MarkupLMTokenizer', 'MBartTokenizer', 'MBart50Tokenizer', 'MPNetTokenizer', 'MobileBertTokenizer', 'MvpTokenizer', 'NllbTokenizer', 'OpenAIGPTTokenizer', 'PegasusTokenizer', 'Qwen2Tokenizer', 'RealmTokenizer', 'ReformerTokenizer', 'RemBertTokenizer', 'RetriBertTokenizer', 'RobertaTokenizer', 'RoFormerTokenizer', 'SeamlessM4TTokenizer', 'SqueezeBertTokenizer', 'T5Tokenizer', 'UdopTokenizer', 'WhisperTokenizer', 'XLMRobertaTokenizer', 'XLNetTokenizer', 'SplinterTokenizer', 'XGLMTokenizer', 'LlamaTokenizer', 'CodeLlamaTokenizer', 'GemmaTokenizer', 'Phi3Tokenizer']"
      ]
    },
    "tinyllama-1.1b": {
      "status": "all_benchmarks_failed",
      "model_info": {
        "repo_id": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
        "size_gb": 1.0,
        "files": [],
        "path": "models\\tinyllama-1.1b",
        "type": "standard",
        "status": "partial",
        "message": "Standard model download needs implementation"
      },
      "benchmark_count": 1,
      "errors": [
        "Converting from SentencePiece and Tiktoken failed, if a converter for SentencePiece is available, provide a model path with a SentencePiece tokenizer.model file.Currently available slow->fast converters: ['AlbertTokenizer', 'BartTokenizer', 'BarthezTokenizer', 'BertTokenizer', 'BigBirdTokenizer', 'BlenderbotTokenizer', 'CamembertTokenizer', 'CLIPTokenizer', 'CodeGenTokenizer', 'ConvBertTokenizer', 'DebertaTokenizer', 'DebertaV2Tokenizer', 'DistilBertTokenizer', 'DPRReaderTokenizer', 'DPRQuestionEncoderTokenizer', 'DPRContextEncoderTokenizer', 'ElectraTokenizer', 'FNetTokenizer', 'FunnelTokenizer', 'GPT2Tokenizer', 'HerbertTokenizer', 'LayoutLMTokenizer', 'LayoutLMv2Tokenizer', 'LayoutLMv3Tokenizer', 'LayoutXLMTokenizer', 'LongformerTokenizer', 'LEDTokenizer', 'LxmertTokenizer', 'MarkupLMTokenizer', 'MBartTokenizer', 'MBart50Tokenizer', 'MPNetTokenizer', 'MobileBertTokenizer', 'MvpTokenizer', 'NllbTokenizer', 'OpenAIGPTTokenizer', 'PegasusTokenizer', 'Qwen2Tokenizer', 'RealmTokenizer', 'ReformerTokenizer', 'RemBertTokenizer', 'RetriBertTokenizer', 'RobertaTokenizer', 'RoFormerTokenizer', 'SeamlessM4TTokenizer', 'SqueezeBertTokenizer', 'T5Tokenizer', 'UdopTokenizer', 'WhisperTokenizer', 'XLMRobertaTokenizer', 'XLNetTokenizer', 'SplinterTokenizer', 'XGLMTokenizer', 'LlamaTokenizer', 'CodeLlamaTokenizer', 'GemmaTokenizer', 'Phi3Tokenizer']"
      ]
    },
    "phi-3.5-vision-instruct": {
      "status": "all_benchmarks_failed",
      "model_info": {
        "repo_id": "microsoft/Phi-3.5-vision-instruct",
        "size_gb": 1.0,
        "files": [],
        "path": "models\\phi-3.5-vision-instruct",
        "type": "standard",
        "status": "partial",
        "message": "Standard model download needs implementation"
      },
      "benchmark_count": 1,
      "errors": [
        "expected str, bytes or os.PathLike object, not NoneType"
      ]
    }
  },
  "performance_comparison": {
    "error": "No successful models to compare"
  }
}